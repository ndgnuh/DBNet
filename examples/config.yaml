# image_size (typing.Tuple[int, int]):
#	The size of the input image.
#	The value must be a tuple of (width, height).
image_size:
- 1024
- 1024

# hidden_size (int):
#	The number of output channels for the feature pyramid module.
#	The number of channels of prediction head will be 1/4 of this.
hidden_size: 256

# classes (typing.List[str]):
#	The object classes.
#	Mapping from class index to string is required for pretty printting
#	and for reverse mapping from dataset files.
#	The model only cares about how many class there are.
classes:
- dog
- cat

# backbone (str):
#	Model backbone
#	Supported values: mobilenet_v3_large, mobilenet_v3_small
backbone: mobilenet_v3_large

# target_size (typing.Optional[typing.Tuple[int, int]]):
#	The size of the target heatmap.
#	Can either be a tuple of (width, height) or null.
#	If null, `image_size` will be used.
target_size: null

# max_distance (typing.Union[int, float, NoneType]):
#	The maximum shrinking distance for the bounding boxes.
#	None value means no maximum.
#	Integer value means absolute maximum value.
#	Float value means the maximum will be some percentage of the target area.
max_distance: null

# min_box_size (typing.Union[int, float, NoneType]):
#	The minimum bounding box size.
#	None value means no minimum size.
#	Integer value means absolute minimum size.
#	Float value means the minimum size will be some percentage of the target area.
min_box_size: null

# shrink (bool):
#	If `True`, the polygons are shrunken when drawing the probablity maps.
#	If `False`, the original polygons are used.
shrink: true

# shrink_rate (float):
#	DBNet shrink ratio from DBNet paper. The shrink will be
#	A * (1 - r^2) / L, where r is the ratio, A and L are the
#	area and the length of the polygon bounding box.
shrink_rate: 0.4

# expand_rate (float):
#	DBNet expand ratio from DBNet paper. The expand distance will be
#	A * r / L, where r is the ratio, A and L are the area
#	and the length of the polygon bounding box.
expand_rate: 1.5

# train_data (typing.Optional[str]):
#	Path to training data lmdb directory.
#	Not used in inference process.
#	Must not be null if training.
train_data: examples/lmdb/train/

# src_train_data (typing.Optional[str]):
#	Path to train data index.
#	This is used to create LMDB dataset.
src_train_data: examples/train.txt

# val_data (typing.Optional[str]):
#	Path to validation data lmdb directory.
#	Not used in inference process.
#	Must not be null if training.
val_data: examples/lmdb/val/

# src_val_data (typing.Optional[str]):
#	Path to validate data index.
#	This is used to create LMDB dataset.
src_val_data: examples/val.txt

# learning_rate (float):
#	Training learning rate
learning_rate: 0.007

# learning_rate_schedule (str):
#	Learning rate scheduler to use. Not yet implemented.
#	Supported values: dbnet
learning_rate_schedule: dbnet

# total_steps (int):
total_steps: 100000

# print_every (int):
print_every: 250

# validate_every (int):
validate_every: 1000

# batch_size (int):
batch_size: 1

# num_workers (int):
num_workers: 0

# augment_enabled (bool):
#	Whether to enable data augmentation when training
augment_enabled: true

# augment_prob (float):
#	Augmentation apply probability for each transformation
augment_prob: 0.3

# augment_rotate (bool):
#	Whether to apply rotation augmentation
augment_rotate: true

# augment_flip (bool):
#	Whether to apply flipping augmentation
augment_flip: false

# weight_path (typing.Optional[str]):
#	Model weight load path.
#	The weight will be loaded in training mode,
#	inference mode, and while exporting to ONNX.
weight_path: null

# latest_weight_path (typing.Optional[str]):
#	Model latest weight save path.
#	This will be used in training mode to save every n steps.
latest_weight_path: latest.pt

# best_weight_path (typing.Optional[str]):
#	Model best weight save path.
#	This will be used in training mode to save
#	when the evalation metric improves.
best_weight_path: best.pt

# onnx_path (typing.Optional[str]):
#	Model onnx path.
#	This will be used for ONNX export and ONNX predictor.
onnx_path: model.onnx
